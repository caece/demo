{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué vamos a hacer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Loan Modelling (Caso de uso)\n",
    "\n",
    "Un banco tiene dos categorías de clientes:\n",
    "  - Depositors (dinero depositado en el banco)\n",
    "  - Borrowers (pidieron un préstamo) <br>\n",
    "Estas categorías NO son mutuamente excluyentes. <br>\n",
    "\n",
    "La gran mayoría de los clientes son Depositors, por lo que el banco quiere hacer crecer la cantidad de Borrowers. Para esto, el banco quiere hacer crecer su base de Borrowers entre los Depositors (quiere aumentar la cantidad de productos por cliente).<br><br>\n",
    "Para esto, el departamento de marketing lanzó una nueva campaña de promoción de préstamos personales. Se hizo una prueba piloto con 5000 clientes y se tuvieron buenos resultados, con una tasa de conversión de aproximadamente el 10%\n",
    "<br><br>\n",
    "Basado en los resultados de esta campaña, el departamento de ventas quiere hacer un modelo que pueda predecir si una persona va a tomar un préstamos personal o no, de manera de concentrar sus esfuerzos en los clientes con mayor probabilidad de aceptación. El objetivo general es llevar a cabo la campaña a gran escala con el menor costo posible. (cada cliente que llamo, es un costo. La idea es llamar la menor cantidad de gente posible. Se busca alta tasa de exito, reduciendo los casos totales (y no aumentando los casos de exito)). (Entonces, no quieren salir a enchufarle créditos a todo el mundo. Lo que quieren es saber de antemano quién va a decir que si, para poder concentrar sus esfuerzos en ellos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo lo vamos a hacer?\n",
    "#### ¿De dónde a dónde estamos yendo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear Spark Session\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('caece').config(conf=SparkConf()).getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://caece-public.obs.la-south-2.myhuaweicloud.com/Bank_Personal_Loan_Modelling_2.csv\"\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "sc.addFile(url)\n",
    "csv_a_cargar = \"file://\"+SparkFiles.get(\"Bank_Personal_Loan_Modelling_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spark DataFrame\n",
    "spark_df = spark.read.options(header='True', inferSchema='True', delimiter='\\t').\\\n",
    "csv(csv_a_cargar)\n",
    "spark_df = spark_df.drop('Online', 'CreditCard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chequeos Básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos el objeto (spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Edad: int, Experiencia: int, Ingresos: int, Cod_Postal: int, Familia: int, Tarjeta Credito: double, Educacion: int, Valor Hipoteca: int, Cta Comitente: int, Plazo Fijo: int, Prestamo: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()  # Cantidad de registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ID\n",
      "1 - Edad\n",
      "2 - Experiencia\n",
      "3 - Ingresos\n",
      "4 - Cod_Postal\n",
      "5 - Familia\n",
      "6 - Tarjeta Credito\n",
      "7 - Educacion\n",
      "8 - Valor Hipoteca\n",
      "9 - Cta Comitente\n",
      "10 - Plazo Fijo\n",
      "11 - Prestamo\n"
     ]
    }
   ],
   "source": [
    "# Nombres de las columnas \n",
    "for idx, element in enumerate(spark_df.columns):\n",
    "    print(idx, '-', element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+--------+----------+-------+---------------+---------+--------------+-------------+----------+--------+\n",
      "| ID|Edad|Experiencia|Ingresos|Cod_Postal|Familia|Tarjeta Credito|Educacion|Valor Hipoteca|Cta Comitente|Plazo Fijo|Prestamo|\n",
      "+---+----+-----------+--------+----------+-------+---------------+---------+--------------+-------------+----------+--------+\n",
      "|  1|  25|          1|      49|     91107|      4|            1.6|        1|             0|            1|         0|       0|\n",
      "|  2|  45|         19|      34|     90089|      3|            1.5|        1|             0|            1|         0|       0|\n",
      "|  3|  39|         15|      11|     94720|      1|            1.0|        1|             0|            0|         0|       0|\n",
      "+---+----+-----------+--------+----------+-------+---------------+---------+--------------+-------------+----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(3)  # Muestra de 3 filas de la tabla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mejor visualización, vamos a usar la acción toPandas(). <br>\n",
    "Esto es sólo para visualización. NO significa que Spark DF y Pandas DF sean lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   1    25            1        49       91107        4              1.6   \n",
       "1   2    45           19        34       90089        3              1.5   \n",
       "2   3    39           15        11       94720        1              1.0   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \n",
       "0          1               0              1           0         0  \n",
       "1          1               0              1           0         0  \n",
       "2          1               0              0           0         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un poco de estadística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>25%</td>\n",
       "      <td>50%</td>\n",
       "      <td>75%</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>5000</td>\n",
       "      <td>2500.5</td>\n",
       "      <td>1443.5200033252052</td>\n",
       "      <td>1</td>\n",
       "      <td>1250</td>\n",
       "      <td>2500</td>\n",
       "      <td>3750</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edad</th>\n",
       "      <td>5000</td>\n",
       "      <td>45.3384</td>\n",
       "      <td>11.463165630542662</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiencia</th>\n",
       "      <td>5000</td>\n",
       "      <td>20.1046</td>\n",
       "      <td>11.46795368112056</td>\n",
       "      <td>-3</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ingresos</th>\n",
       "      <td>5000</td>\n",
       "      <td>73.7742</td>\n",
       "      <td>46.03372932108627</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>64</td>\n",
       "      <td>98</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cod_Postal</th>\n",
       "      <td>5000</td>\n",
       "      <td>93152.503</td>\n",
       "      <td>2121.8521973361953</td>\n",
       "      <td>9307</td>\n",
       "      <td>91911</td>\n",
       "      <td>93437</td>\n",
       "      <td>94608</td>\n",
       "      <td>96651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Familia</th>\n",
       "      <td>5000</td>\n",
       "      <td>2.3964</td>\n",
       "      <td>1.1476630455378507</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.9379380000000053</td>\n",
       "      <td>1.7476589800467675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Educacion</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.881</td>\n",
       "      <td>0.839869082664199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <td>5000</td>\n",
       "      <td>56.4988</td>\n",
       "      <td>101.71380210211213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cta Comitente</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.30580932600032634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.23825027311322794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prestamo</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.29462070577617994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                   1                    2     3      4  \\\n",
       "summary          count                mean               stddev   min    25%   \n",
       "ID                5000              2500.5   1443.5200033252052     1   1250   \n",
       "Edad              5000             45.3384   11.463165630542662    23     35   \n",
       "Experiencia       5000             20.1046    11.46795368112056    -3     10   \n",
       "Ingresos          5000             73.7742    46.03372932108627     8     39   \n",
       "Cod_Postal        5000           93152.503   2121.8521973361953  9307  91911   \n",
       "Familia           5000              2.3964   1.1476630455378507     1      1   \n",
       "Tarjeta Credito   5000  1.9379380000000053   1.7476589800467675   0.0    0.7   \n",
       "Educacion         5000               1.881    0.839869082664199     1      1   \n",
       "Valor Hipoteca    5000             56.4988   101.71380210211213     0      0   \n",
       "Cta Comitente     5000              0.1044  0.30580932600032634     0      0   \n",
       "Plazo Fijo        5000              0.0604  0.23825027311322794     0      0   \n",
       "Prestamo          5000               0.096  0.29462070577617994     0      0   \n",
       "\n",
       "                     5      6      7  \n",
       "summary            50%    75%    max  \n",
       "ID                2500   3750   5000  \n",
       "Edad                45     55     67  \n",
       "Experiencia         20     30     43  \n",
       "Ingresos            64     98    224  \n",
       "Cod_Postal       93437  94608  96651  \n",
       "Familia              2      3      4  \n",
       "Tarjeta Credito    1.5    2.5   10.0  \n",
       "Educacion            2      3      3  \n",
       "Valor Hipoteca       0    101    635  \n",
       "Cta Comitente        0      0      1  \n",
       "Plazo Fijo           0      0      1  \n",
       "Prestamo             0      0      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.summary().toPandas().transpose()  # Summary me da las estadísticas del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Spark se maneja todo con vectores. Entonces, para armar la matriz de correlación, es necesario transformar nuestros registros en un único vector. Esto lo hacemos con un objeto VectorAssembler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorAssembler paso a paso.<br>\n",
    "Lo que vamos a hacer va a ser transformar las filas (registros) en un vector, para meterlo en la matriz de correlación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   1    25            1        49       91107        4              1.6   \n",
       "1   2    45           19        34       90089        3              1.5   \n",
       "2   3    39           15        11       94720        1              1.0   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \n",
       "0          1               0              1           0         0  \n",
       "1          1               0              1           0         0  \n",
       "2          1               0              0           0         0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df original\n",
    "spark_df.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el objeto VectorAssembler (una máquina que crea vectores a partir de las columnas de input)<br>\n",
    "Columnas ---> VectorAssembler ---> Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen columnas de entrada y de salida. En este caso le estoy pasando todas las columnas\n",
    "corr_assembler = VectorAssembler(inputCols=spark_df.columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando use 'corr_assembler' se va a crear un nuevo df de spark que tiene todas las columnas de entrada más una columna que contiene al vector generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 25.0, 1.0, 49.0, 91107.0, 4.0, 1.6, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.0, 45.0, 19.0, 34.0, 90089.0, 3.0, 1.5, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[3.0, 39.0, 15.0, 11.0, 94720.0, 1.0, 1.0, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   1    25            1        49       91107        4              1.6   \n",
       "1   2    45           19        34       90089        3              1.5   \n",
       "2   3    39           15        11       94720        1              1.0   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \\\n",
       "0          1               0              1           0         0   \n",
       "1          1               0              1           0         0   \n",
       "2          1               0              0           0         0   \n",
       "\n",
       "                                            features  \n",
       "0  [1.0, 25.0, 1.0, 49.0, 91107.0, 4.0, 1.6, 1.0,...  \n",
       "1  [2.0, 45.0, 19.0, 34.0, 90089.0, 3.0, 1.5, 1.0...  \n",
       "2  [3.0, 39.0, 15.0, 11.0, 94720.0, 1.0, 1.0, 1.0...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se le agrega una columna (la columna vector) al df original. \n",
    "corr_df = corr_assembler.transform(spark_df)\n",
    "corr_df.toPandas()[:3]  # Mismo df, pero con una columna 'features' que tiene el vector que vamos meter en Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se toma únicamente la columna del vector ('features')\n",
    "corr_vector = corr_df.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 25.0, 1.0, 49.0, 91107.0, 4.0, 1.6, 1.0, 0.0, 1.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([2.0, 45.0, 19.0, 34.0, 90089.0, 3.0, 1.5, 1.0, 0.0, 1.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([3.0, 39.0, 15.0, 11.0, 94720.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_vector.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, podemos crear nuestra matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y se crea la matriz de correlación. \n",
    "corr_matrix = Correlation.corr(corr_vector, 'features') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro assembled vector tiene la data de todas las columnas anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la matriz de correlación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t -0.008\t -0.008\t -0.018\t 0.013\t -0.017\t -0.025\t 0.021\t -0.014\t -0.017\t -0.007\t -0.025\t \n",
      "-0.008\t 1.0\t 0.994\t -0.055\t -0.029\t -0.046\t -0.052\t 0.041\t -0.013\t -0.0\t 0.008\t -0.008\t \n",
      "-0.008\t 0.994\t 1.0\t -0.047\t -0.029\t -0.053\t -0.05\t 0.013\t -0.011\t -0.001\t 0.01\t -0.007\t \n",
      "-0.018\t -0.055\t -0.047\t 1.0\t -0.016\t -0.158\t 0.646\t -0.188\t 0.207\t -0.003\t 0.17\t 0.502\t \n",
      "0.013\t -0.029\t -0.029\t -0.016\t 1.0\t 0.012\t -0.004\t -0.017\t 0.007\t 0.005\t 0.02\t 0.0\t \n",
      "-0.017\t -0.046\t -0.053\t -0.158\t 0.012\t 1.0\t -0.109\t 0.065\t -0.02\t 0.02\t 0.014\t 0.061\t \n",
      "-0.025\t -0.052\t -0.05\t 0.646\t -0.004\t -0.109\t 1.0\t -0.136\t 0.11\t 0.015\t 0.137\t 0.367\t \n",
      "0.021\t 0.041\t 0.013\t -0.188\t -0.017\t 0.065\t -0.136\t 1.0\t -0.033\t -0.011\t 0.014\t 0.137\t \n",
      "-0.014\t -0.013\t -0.011\t 0.207\t 0.007\t -0.02\t 0.11\t -0.033\t 1.0\t -0.005\t 0.089\t 0.142\t \n",
      "-0.017\t -0.0\t -0.001\t -0.003\t 0.005\t 0.02\t 0.015\t -0.011\t -0.005\t 1.0\t 0.317\t 0.022\t \n",
      "-0.007\t 0.008\t 0.01\t 0.17\t 0.02\t 0.014\t 0.137\t 0.014\t 0.089\t 0.317\t 1.0\t 0.316\t \n",
      "-0.025\t -0.008\t -0.007\t 0.502\t 0.0\t 0.061\t 0.367\t 0.137\t 0.142\t 0.022\t 0.316\t 1.0\t \n"
     ]
    }
   ],
   "source": [
    "for element in corr_matrix.collect()[0][0].toArray():\n",
    "    to_print = ''\n",
    "    for num in element:\n",
    "        to_print += (str(round(num, 3)) + '\\t ') \n",
    "    print(to_print)\n",
    "    \n",
    "# Hay que mirar la tercera desde la izquierda... Sé que no es la mejor visualización... pero es lo que hay. \n",
    "# 3, 6, 7, 8, 11\n",
    "# Mejorar display!  Mostrar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t -0.0248 \t ID\n",
      "1 \t -0.00773 \t Edad\n",
      "2 \t -0.00741 \t Experiencia\n",
      "3 \t 0.50246 \t Ingresos\n",
      "4 \t 0.00011 \t Cod_Postal\n",
      "5 \t 0.06137 \t Familia\n",
      "6 \t 0.36689 \t Tarjeta Credito\n",
      "7 \t 0.13672 \t Educacion\n",
      "8 \t 0.1421 \t Valor Hipoteca\n",
      "9 \t 0.02195 \t Cta Comitente\n",
      "10 \t 0.31635 \t Plazo Fijo\n",
      "11 \t 1.0 \t Prestamo\n"
     ]
    }
   ],
   "source": [
    "for idx, element in enumerate(corr_matrix.collect()[0][0].toArray()):\n",
    "    print(idx, '\\t', round(element[-1], 5),'\\t',spark_df.columns[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hay mucho más para hacer en cuanto a exploración de datos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podría ser un buen predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Ingresos\n",
      "1 - Tarjeta Credito\n",
      "2 - Educacion\n",
      "3 - Valor Hipoteca\n",
      "4 - Plazo Fijo\n"
     ]
    }
   ],
   "source": [
    "reduced_df = spark_df['ID', 'Ingresos', 'Tarjeta Credito', 'Educacion', 'Valor Hipoteca', 'Plazo Fijo','Prestamo']\n",
    "feature_columns = [col for col in reduced_df.columns if ((col != 'ID') & (col != 'Prestamo'))]\n",
    "\n",
    "for idx, element in enumerate(feature_columns):\n",
    "    print(idx, '-', element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------------+---------+--------------+----------+--------+\n",
      "| ID|Ingresos|Tarjeta Credito|Educacion|Valor Hipoteca|Plazo Fijo|Prestamo|\n",
      "+---+--------+---------------+---------+--------------+----------+--------+\n",
      "|  1|      49|            1.6|        1|             0|         0|       0|\n",
      "|  2|      34|            1.5|        1|             0|         0|       0|\n",
      "|  3|      11|            1.0|        1|             0|         0|       0|\n",
      "+---+--------+---------------+---------+--------------+----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced_df.show(3)  # Ahora si podemos usar .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoría. Logistic Regression\n",
    "#### ¿Qué es?\n",
    "#### ¿Cómo funciona?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "#### Feature Scaling\n",
    "Para poder meter todas las features en la misma bolsa, es necesario normalizar las variables cuantitativas. <br>\n",
    "En este caso, tenemos variables a normalizar:\n",
    "- Ingresos\n",
    "- Gasto promedio con Tajeta de Crédito\n",
    "- Valor de la hipoteca\n",
    "- Educación* \n",
    "<br>\n",
    "Técnicamente, Educación es una variable categórica. Pero como son 3 niveles y va a menor a mayor, nos sirve normalizarla para que el nivel quede entre 0 y 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para escalar las features, vamos a usar la clase MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Va a ser necesario crear un assembler por cada variable:<br>\n",
    "No es como antes que podemos meterle todas las columnas y sacar un solo vector, porque cada columna va a necesitar su Scaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que antes. Creamos el Assembler, y transformamos el df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_assembler = VectorAssembler(inputCols=['Ingresos'], outputCol=\"vec_Ingresos\")\n",
    "ing_assembled = ing_assembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá tenemos un df intermedio \"ing_assembled\"<br>\n",
    "Ahora creamos el objeto MinMaxScaler, que va a transformar el df ing_assembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_scaler = MinMaxScaler(inputCol=\"vec_Ingresos\", outputCol=\"sc_Ingresos\")\n",
    "ing_scaler_model = ing_scaler.fit(ing_assembled)\n",
    "ing_df = ing_scaler_model.transform(ing_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así queda nuesta feature original, nuestra feature vectorizada y nuestra feature vectorizada y escalada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+\n",
      "|Ingresos|vec_Ingresos|         sc_Ingresos|\n",
      "+--------+------------+--------------------+\n",
      "|      49|      [49.0]|[0.1898148148148148]|\n",
      "|      34|      [34.0]|[0.12037037037037...|\n",
      "|      11|      [11.0]|[0.01388888888888...|\n",
      "|     100|     [100.0]|[0.42592592592592...|\n",
      "|      45|      [45.0]|[0.17129629629629...|\n",
      "+--------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ing_df.select('Ingresos', 'vec_Ingresos', 'sc_Ingresos').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que hacer lo mismo para las otras 3 features...<br>\n",
    "Vamos a necesitar una forma más eficiente de hacer esto<br><br>\n",
    "Hagamos uso de la clase Pipeline de Spark que nos permiten combinar funciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "columns_to_scale = ['Educacion', 'Tarjeta Credito', 'Valor Hipoteca']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la clase Pipeline podemos hacer todas las transformaciones juntas. <br>\n",
    "Nos permite:\n",
    "- Aprovechar la lazy evaluation de Spark\n",
    "- Deshacerme de los pipelines intermedios\n",
    "- Ahorrar algunas líneas de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean todos los assemblers\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=\"vec_\" + col) for col in columns_to_scale]\n",
    "# Se crean todos los scalers\n",
    "scalers = [MinMaxScaler(inputCol=\"vec_\" + col, outputCol=\"sc_\" + col) for col in columns_to_scale]\n",
    "# Se crea el pipeline. se definen las etapas del pipeline: primero todos los assemblers, después todos los scalers. \n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "# GRAN VENTAJA DEL PIPELINE: Me permite deshacerme de los dfs intermedios. \n",
    "scalerModel = pipeline.fit(ing_df)  # lo que hay que fittear son los scalers. \n",
    "scaledData = scalerModel.transform(ing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledData.select('Tarjeta Credito', 'vec_Tarjeta Credito', 'sc_Tarjeta Credito',\n",
    "                  'Educacion', 'vec_Educacion', 'sc_Educacion').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien. Todo listo para entrenar el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separo los datos en un set de entrenamiento y de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = scaledData.randomSplit([0.8,0.2])  # 80% training - 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Está bien esto?\n",
    "¿Qué representa mi set de testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage\n",
    "Cuando escalé las features, usé el set completo. El algoritmo tuvo acceso a información \"del futuro\". En este caso no afecta demasiado... pero el Data Leakage es algo importante a tener en cuenta <br>\n",
    "Como regla general:\n",
    "- Para mirar los datos, podemos usar todos\n",
    "- Para manipularlos, primero hay que separar el test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split - Scaling do over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primero el Split, después el Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "training_data, test_data = spark_df.randomSplit([0.8,0.2])  # 80% training - 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se repiten los pasos de antes. Pero el MinMaxScaler se entrena sólo con mi training set. Después se escala la data por separado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \n",
    "# Hacemos exactamente lo mismo que antes\n",
    "columns_to_scale = ['Ingresos', 'Educacion', 'Tarjeta Credito', 'Valor Hipoteca']\n",
    "\n",
    "# Se crean todos los assemblers IGUAL QUE ANTES\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=\"vec_\" + col) for col in columns_to_scale]\n",
    "# Se crean todos los scalers IGUAL QUE ANTES\n",
    "scalers = [MinMaxScaler(inputCol=\"vec_\" + col, outputCol=\"sc_\" + col) for col in columns_to_scale]\n",
    "# Se crea el pipeline. IGUAL QUE ANTES\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "\n",
    "scaler_model = pipeline.fit(training_data)\n",
    "# hago la normalización para los datos de training y los de test\n",
    "sc_training_df = scaler_model.transform(training_data)\n",
    "sc_test_df = scaler_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_training_df.toPandas()[:3]  # To Pandas es solo para visualización. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo, vamos a necesitar un vector similar al que usamos antes para la matriz de correlación. <br>\n",
    "Primero vamos a necesitar pasar las features normalizadas de vector a numero.<br>\n",
    "Spark parece muy complicado... comparación con Pandas. (exploratorio vs productivo. Todo lo que se hace acá es para mostrar, pero se podría hacer en un pipeline sin necesidad de mostrar resultados intermedios. Spark no remplaza a Pandas, cumplen funciones distintas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se pasan las features normalizadas de vector a float...\n",
    "# Para poder transformarlas en un solo vector. \n",
    "from pyspark.ml.functions import vector_to_array\n",
    "sc_training_df.withColumn(\"num_sc_Ingresos\", vector_to_array(\"sc_Ingresos\")[0])\\\n",
    ".withColumn('num_sc_Educacion', vector_to_array('sc_Educacion')[0])\\\n",
    ".withColumn('num_sc_Tarjeta Credito', vector_to_array('sc_Tarjeta Credito')[0])\\\n",
    ".withColumn('num_sc_Valor Hipoteca', vector_to_array('sc_Valor Hipoteca')[0])\\\n",
    ".select('ID', \"num_sc_Ingresos\", \"num_sc_Educacion\", \"num_sc_Tarjeta Credito\", \"num_sc_Valor Hipoteca\", \"Plazo Fijo\", \"Prestamo\")\\\n",
    ".toPandas()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sc_training_df.withColumn(\"num_sc_Ingresos\", vector_to_array(\"sc_Ingresos\")[0])\\\n",
    ".withColumn('num_sc_Educacion', vector_to_array('sc_Educacion')[0])\\\n",
    ".withColumn('num_sc_Tarjeta Credito', vector_to_array('sc_Tarjeta Credito')[0])\\\n",
    ".withColumn('num_sc_Valor Hipoteca', vector_to_array('sc_Valor Hipoteca')[0])\\\n",
    ".select('ID', \"num_sc_Ingresos\", \"num_sc_Educacion\", \"num_sc_Tarjeta Credito\", \"num_sc_Valor Hipoteca\", \"Plazo Fijo\", \"Prestamo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in training_set.columns if (col not in ['ID', 'Prestamo'] )]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos, con VectorAssembler, un vector que contenga nuestras features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vec_features\")\n",
    "# transformar sc_training_df en algo que tenga una columna más. \n",
    "vec_training_set = vec_assembler.transform(training_set).select('vec_features', 'Prestamo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_training_set.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo LogisticRegression y lo entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Prestamo\", featuresCol=\"vec_features\", maxIter=10)\n",
    "lr_pipeline = Pipeline(stages=[lr])  # Para una sola transformación, no es necesario el pipeline. \n",
    "# Pero, se puede combinar con la transformación anterior (columnas a vectores, para hacer el pipeline)\n",
    "\n",
    "# Con fit, entrenamos el modelo\n",
    "lr_model = lr_pipeline.fit(vec_training_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos el test set de la misma manera que transformamos el anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = sc_test_df.withColumn(\"num_sc_Ingresos\", vector_to_array(\"sc_Ingresos\")[0])\\\n",
    ".withColumn('num_sc_Educacion', vector_to_array('sc_Educacion')[0])\\\n",
    ".withColumn('num_sc_Tarjeta Credito', vector_to_array('sc_Tarjeta Credito')[0])\\\n",
    ".withColumn('num_sc_Valor Hipoteca', vector_to_array('sc_Valor Hipoteca')[0])\\\n",
    ".select('ID', \"num_sc_Ingresos\", \"num_sc_Educacion\", \"num_sc_Tarjeta Credito\", \"num_sc_Valor Hipoteca\", \"Plazo Fijo\", \"Prestamo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_test_set = vec_assembler.transform(test_set).select('vec_features', 'Prestamo')\n",
    "vec_test_set.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = lr_model.transform(vec_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('vec_features', 'rawPrediction', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.where(predictions['prediction'] == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.where(predictions['prediction'] == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces... Esto es bueno? Esto es malo... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué es lo que no estamos viendo acá? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Si la predicción es correcta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('vec_features', 'rawPrediction', 'prediction', 'Prestamo').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué es? Para qué se usa? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir nuestra Confusion Matrix usando la clase MultiClassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos el tipo de dato de la columna 'Prestamo', de int a float (porque asi lo necesita nuestro objeto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "predictions_labels = predictions['prediction', 'Prestamo']\\\n",
    ".withColumn('Prestamo', col('Prestamo').cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que nos quedamos únicamente con las columnas prediction y Prestamo<br>\n",
    "Creamos un objeto 'metrics', a partir de MultiClassMetrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(predictions_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué le pasamos a esta clase para instanciarla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels.rdd.map(tuple).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pasamos una lista de tuples (prediccion, Prestamo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusionMatrix().toArray().transpose()\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a guardar estos resultados para corroborar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix[1][1]\n",
    "tn = confusion_matrix[0][0]\n",
    "fp = confusion_matrix[1][0]\n",
    "fn = confusion_matrix[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tp + tn) / (tp + tn + fp + fn)  # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94% suena bien, no?\n",
    "<br><br><br>..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el caso de un clasificador que siempre predice 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_positive = tp + fn\n",
    "total_negative = tn + fp\n",
    "total_cases = total_positive + total_negative\n",
    "\n",
    "print('Dummy accuracy: {}'.format((total_negative / (total_cases))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces: Nuestro clasificador es bueno, pero no taaan bueno. \n",
    "<br> Moraleja: No juzgar porcentajes... hay que ver todas las métricas\n",
    "<br> Otras métricas: \n",
    "- Precision\n",
    "- Recall\n",
    "- La métrica más importante para clasificadores binarios es el área bajo la ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo: Minority Report - Ladrones e Inocentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision(1.0)\n",
    "precision\n",
    "# busco minimizar fp (no encarcelar inocentes, aunque queden ladrones libres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tp) / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = metrics.recall(1.0)\n",
    "recall\n",
    "# busco minimizar fn (encarcelar todos los ladrones, a expensas de encarcelar algún inocente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tp) / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curve (AUC ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "receiver operating characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "bin_metrics = BinaryClassificationMetrics(predictions_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ya tenemos una primera iteración. Tenemos un modelo funcionando. Se puede mejorar el modelo metiéndole horas y cabeza.\n",
    "Cosas para ajustar:\n",
    "- Agregar, sacar o modificar features\n",
    "- Peso de las features.\n",
    "- Peso de las clases\n",
    "- Parámetros del Modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL)\n",
    "## Ethics in Data Science and ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
