{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué vamos a hacer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Loan Modelling (Caso de uso)\n",
    "\n",
    "Un banco tiene dos categorías de clientes:\n",
    "  - Depositors (dinero depositado en el banco)\n",
    "  - Borrowers (pidieron un préstamo) <br>\n",
    "Estas categorías NO son mutuamente excluyentes. <br>\n",
    "\n",
    "La gran mayoría de los clientes son Depositors, por lo que el banco quiere hacer crecer la cantidad de Borrowers. Para esto, el banco quiere hacer crecer su base de Borrowers entre los Depositors (quiere aumentar la cantidad de productos por cliente).<br><br>\n",
    "Para esto, el departamento de marketing lanzó una nueva campaña de promoción de préstamos personales. Se hizo una prueba piloto con 5000 clientes y se tuvieron buenos resultados, con una tasa de conversión de aproximadamente el 10%\n",
    "<br><br>\n",
    "Basado en los resultados de esta campaña, el departamento de ventas quiere hacer un modelo que pueda predecir si una persona va a tomar un préstamos personal o no, de manera de concentrar sus esfuerzos en los clientes con mayor probabilidad de aceptación. El objetivo general es llevar a cabo la campaña a gran escala con el menor costo posible. (cada cliente que llamo, es un costo. La idea es llamar la menor cantidad de gente posible. Se busca alta tasa de exito, reduciendo los casos totales (y no aumentando los casos de exito)). (Entonces, no quieren salir a enchufarle créditos a todo el mundo. Lo que quieren es saber de antemano quién va a decir que si, para poder concentrar sus esfuerzos en ellos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo lo vamos a hacer?\n",
    "#### ¿De dónde a dónde estamos yendo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SetUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurar PySpark para levantar tabla de Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear Spark Session\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('caece').config(conf=SparkConf()).getOrCreate()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spark DataFrame\n",
    "spark_df = spark.read.options(header='True', inferSchema='True', delimiter='\\t').\\\n",
    "csv('file:/home/jovyan/work/CAECE/Bank_Personal_Loan_Modelling_2.csv')\n",
    "spark_df = spark_df.drop('Online', 'CreditCard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chequeos Básicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()  # Cantidad de registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ID\n",
      "1 - Edad\n",
      "2 - Experiencia\n",
      "3 - Ingresos\n",
      "4 - Cod_Postal\n",
      "5 - Familia\n",
      "6 - Tarjeta Credito\n",
      "7 - Educacion\n",
      "8 - Valor Hipoteca\n",
      "9 - Cta Comitente\n",
      "10 - Plazo Fijo\n",
      "11 - Prestamo\n"
     ]
    }
   ],
   "source": [
    "# Nombres de las columnas \n",
    "for idx, element in enumerate(spark_df.columns):\n",
    "    print(idx, '-', element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+--------+----------+-------+---------------+---------+--------------+-------------+----------+--------+\n",
      "| ID|Edad|Experiencia|Ingresos|Cod_Postal|Familia|Tarjeta Credito|Educacion|Valor Hipoteca|Cta Comitente|Plazo Fijo|Prestamo|\n",
      "+---+----+-----------+--------+----------+-------+---------------+---------+--------------+-------------+----------+--------+\n",
      "|  1|  25|          1|      49|     91107|      4|            1.6|        1|             0|            1|         0|       0|\n",
      "|  2|  45|         19|      34|     90089|      3|            1.5|        1|             0|            1|         0|       0|\n",
      "|  3|  39|         15|      11|     94720|      1|            1.0|        1|             0|            0|         0|       0|\n",
      "+---+----+-----------+--------+----------+-------+---------------+---------+--------------+-------------+----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(3)  # Muestra de 3 filas de la tabla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mejor visualización, vamos a usar la acción toPandas(). <br>\n",
    "Esto es sólo para visualización. NO significa que Spark DF y Pandas DF sean lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   1    25            1        49       91107        4              1.6   \n",
       "1   2    45           19        34       90089        3              1.5   \n",
       "2   3    39           15        11       94720        1              1.0   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \n",
       "0          1               0              1           0         0  \n",
       "1          1               0              1           0         0  \n",
       "2          1               0              0           0         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un poco de estadística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>25%</td>\n",
       "      <td>50%</td>\n",
       "      <td>75%</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>5000</td>\n",
       "      <td>2500.5</td>\n",
       "      <td>1443.5200033252052</td>\n",
       "      <td>1</td>\n",
       "      <td>1250</td>\n",
       "      <td>2500</td>\n",
       "      <td>3750</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edad</th>\n",
       "      <td>5000</td>\n",
       "      <td>45.3384</td>\n",
       "      <td>11.463165630542662</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiencia</th>\n",
       "      <td>5000</td>\n",
       "      <td>20.1046</td>\n",
       "      <td>11.46795368112056</td>\n",
       "      <td>-3</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ingresos</th>\n",
       "      <td>5000</td>\n",
       "      <td>73.7742</td>\n",
       "      <td>46.03372932108627</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>64</td>\n",
       "      <td>98</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cod_Postal</th>\n",
       "      <td>5000</td>\n",
       "      <td>93152.503</td>\n",
       "      <td>2121.8521973361953</td>\n",
       "      <td>9307</td>\n",
       "      <td>91911</td>\n",
       "      <td>93437</td>\n",
       "      <td>94608</td>\n",
       "      <td>96651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Familia</th>\n",
       "      <td>5000</td>\n",
       "      <td>2.3964</td>\n",
       "      <td>1.1476630455378507</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.9379380000000053</td>\n",
       "      <td>1.7476589800467675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Educacion</th>\n",
       "      <td>5000</td>\n",
       "      <td>1.881</td>\n",
       "      <td>0.839869082664199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <td>5000</td>\n",
       "      <td>56.4988</td>\n",
       "      <td>101.71380210211213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cta Comitente</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.30580932600032634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.23825027311322794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prestamo</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.29462070577617994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                   1                    2     3      4  \\\n",
       "summary          count                mean               stddev   min    25%   \n",
       "ID                5000              2500.5   1443.5200033252052     1   1250   \n",
       "Edad              5000             45.3384   11.463165630542662    23     35   \n",
       "Experiencia       5000             20.1046    11.46795368112056    -3     10   \n",
       "Ingresos          5000             73.7742    46.03372932108627     8     39   \n",
       "Cod_Postal        5000           93152.503   2121.8521973361953  9307  91911   \n",
       "Familia           5000              2.3964   1.1476630455378507     1      1   \n",
       "Tarjeta Credito   5000  1.9379380000000053   1.7476589800467675   0.0    0.7   \n",
       "Educacion         5000               1.881    0.839869082664199     1      1   \n",
       "Valor Hipoteca    5000             56.4988   101.71380210211213     0      0   \n",
       "Cta Comitente     5000              0.1044  0.30580932600032634     0      0   \n",
       "Plazo Fijo        5000              0.0604  0.23825027311322794     0      0   \n",
       "Prestamo          5000               0.096  0.29462070577617994     0      0   \n",
       "\n",
       "                     5      6      7  \n",
       "summary            50%    75%    max  \n",
       "ID                2500   3750   5000  \n",
       "Edad                45     55     67  \n",
       "Experiencia         20     30     43  \n",
       "Ingresos            64     98    224  \n",
       "Cod_Postal       93437  94608  96651  \n",
       "Familia              2      3      4  \n",
       "Tarjeta Credito    1.5    2.5   10.0  \n",
       "Educacion            2      3      3  \n",
       "Valor Hipoteca       0    101    635  \n",
       "Cta Comitente        0      0      1  \n",
       "Plazo Fijo           0      0      1  \n",
       "Prestamo             0      0      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.summary().toPandas().transpose()  # Summary me da las estadísticas del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Spark se maneja todo con vectores. Entonces, para armar la matriz de correlación, es necesario transformar nuestros registros en un único vector. Esto lo hacemos con un objeto VectorAssembler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorAssembler paso a paso.<br>\n",
    "Lo que vamos a hacer va a ser transformar las filas (registros) en un vector, para meterlo en la matriz de correlación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   1    25            1        49       91107        4              1.6   \n",
       "1   2    45           19        34       90089        3              1.5   \n",
       "2   3    39           15        11       94720        1              1.0   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \n",
       "0          1               0              1           0         0  \n",
       "1          1               0              1           0         0  \n",
       "2          1               0              0           0         0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df original\n",
    "spark_df.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el objeto VectorAssembler (una máquina que crea vectores a partir de las columnas de input)<br>\n",
    "Columnas ---> VectorAssembler ---> Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen columnas de entrada y de salida. En este caso le estoy pasando todas las columnas\n",
    "corr_assembler = VectorAssembler(inputCols=spark_df.columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando use 'corr_assembler' se va a crear un nuevo df de spark que tiene todas las columnas de entrada más una columna que contiene al vector generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 25.0, 1.0, 49.0, 91107.0, 4.0, 1.6, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[2.0, 45.0, 19.0, 34.0, 90089.0, 3.0, 1.5, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[3.0, 39.0, 15.0, 11.0, 94720.0, 1.0, 1.0, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   1    25            1        49       91107        4              1.6   \n",
       "1   2    45           19        34       90089        3              1.5   \n",
       "2   3    39           15        11       94720        1              1.0   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \\\n",
       "0          1               0              1           0         0   \n",
       "1          1               0              1           0         0   \n",
       "2          1               0              0           0         0   \n",
       "\n",
       "                                            features  \n",
       "0  [1.0, 25.0, 1.0, 49.0, 91107.0, 4.0, 1.6, 1.0,...  \n",
       "1  [2.0, 45.0, 19.0, 34.0, 90089.0, 3.0, 1.5, 1.0...  \n",
       "2  [3.0, 39.0, 15.0, 11.0, 94720.0, 1.0, 1.0, 1.0...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se le agrega una columna (la columna vector) al df original. \n",
    "corr_df = corr_assembler.transform(spark_df)\n",
    "corr_df.toPandas()[:3]  # Mismo df, pero con una columna 'features' que tiene el vector que vamos meter en Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se toma únicamente la columna del vector ('features')\n",
    "corr_vector = corr_df.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 25.0, 1.0, 49.0, 91107.0, 4.0, 1.6, 1.0, 0.0, 1.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([2.0, 45.0, 19.0, 34.0, 90089.0, 3.0, 1.5, 1.0, 0.0, 1.0, 0.0, 0.0])),\n",
       " Row(features=DenseVector([3.0, 39.0, 15.0, 11.0, 94720.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_vector.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, podemos crear nuestra matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y se crea la matriz de correlación. \n",
    "corr_matrix = Correlation.corr(corr_vector, 'features') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro assembled vector tiene la data de todas las columnas anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la matriz de correlación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t -0.008\t -0.008\t -0.018\t 0.013\t -0.017\t -0.025\t 0.021\t -0.014\t -0.017\t -0.007\t -0.025\t \n",
      "-0.008\t 1.0\t 0.994\t -0.055\t -0.029\t -0.046\t -0.052\t 0.041\t -0.013\t -0.0\t 0.008\t -0.008\t \n",
      "-0.008\t 0.994\t 1.0\t -0.047\t -0.029\t -0.053\t -0.05\t 0.013\t -0.011\t -0.001\t 0.01\t -0.007\t \n",
      "-0.018\t -0.055\t -0.047\t 1.0\t -0.016\t -0.158\t 0.646\t -0.188\t 0.207\t -0.003\t 0.17\t 0.502\t \n",
      "0.013\t -0.029\t -0.029\t -0.016\t 1.0\t 0.012\t -0.004\t -0.017\t 0.007\t 0.005\t 0.02\t 0.0\t \n",
      "-0.017\t -0.046\t -0.053\t -0.158\t 0.012\t 1.0\t -0.109\t 0.065\t -0.02\t 0.02\t 0.014\t 0.061\t \n",
      "-0.025\t -0.052\t -0.05\t 0.646\t -0.004\t -0.109\t 1.0\t -0.136\t 0.11\t 0.015\t 0.137\t 0.367\t \n",
      "0.021\t 0.041\t 0.013\t -0.188\t -0.017\t 0.065\t -0.136\t 1.0\t -0.033\t -0.011\t 0.014\t 0.137\t \n",
      "-0.014\t -0.013\t -0.011\t 0.207\t 0.007\t -0.02\t 0.11\t -0.033\t 1.0\t -0.005\t 0.089\t 0.142\t \n",
      "-0.017\t -0.0\t -0.001\t -0.003\t 0.005\t 0.02\t 0.015\t -0.011\t -0.005\t 1.0\t 0.317\t 0.022\t \n",
      "-0.007\t 0.008\t 0.01\t 0.17\t 0.02\t 0.014\t 0.137\t 0.014\t 0.089\t 0.317\t 1.0\t 0.316\t \n",
      "-0.025\t -0.008\t -0.007\t 0.502\t 0.0\t 0.061\t 0.367\t 0.137\t 0.142\t 0.022\t 0.316\t 1.0\t \n"
     ]
    }
   ],
   "source": [
    "for element in corr_matrix.collect()[0][0].toArray():\n",
    "    to_print = ''\n",
    "    for num in element:\n",
    "        to_print += (str(round(num, 3)) + '\\t ') \n",
    "    print(to_print)\n",
    "    \n",
    "# Hay que mirar la tercera desde la izquierda... Sé que no es la mejor visualización... pero es lo que hay. \n",
    "# 3, 6, 7, 8, 11\n",
    "# Mejorar display!  Mostrar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar código para mejorar el display y que se vea cuáles son las features que vamos a seleccionar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hay mucho más para hacer en cuanto a exploración de datos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podría ser un buen predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Ingresos\n",
      "1 - Tarjeta Credito\n",
      "2 - Educacion\n",
      "3 - Valor Hipoteca\n",
      "4 - Plazo Fijo\n"
     ]
    }
   ],
   "source": [
    "reduced_df = spark_df['ID', 'Ingresos', 'Tarjeta Credito', 'Educacion', 'Valor Hipoteca', 'Plazo Fijo','Prestamo']\n",
    "feature_columns = [col for col in reduced_df.columns if ((col != 'ID') & (col != 'Prestamo'))]\n",
    "\n",
    "for idx, element in enumerate(feature_columns):\n",
    "    print(idx, '-', element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------------+---------+--------------+----------+--------+\n",
      "| ID|Ingresos|Tarjeta Credito|Educacion|Valor Hipoteca|Plazo Fijo|Prestamo|\n",
      "+---+--------+---------------+---------+--------------+----------+--------+\n",
      "|  1|      49|            1.6|        1|             0|         0|       0|\n",
      "|  2|      34|            1.5|        1|             0|         0|       0|\n",
      "|  3|      11|            1.0|        1|             0|         0|       0|\n",
      "+---+--------+---------------+---------+--------------+----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced_df.show(3)  # Ahora si podemos usar .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoría. Logistic Regression\n",
    "#### ¿Qué es?\n",
    "#### ¿Cómo funciona?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "#### Feature Scaling\n",
    "Para poder meter todas las features en la misma bolsa, es necesario normalizar las variables cuantitativas. <br>\n",
    "En este caso, tenemos variables a normalizar:\n",
    "- Ingresos\n",
    "- Gasto promedio con Tajeta de Crédito\n",
    "- Valor de la hipoteca\n",
    "- Educación* \n",
    "<br>\n",
    "Técnicamente, Educación es una variable categórica. Pero como son 3 niveles y va a menor a mayor, nos sirve normalizarla para que el nivel quede entre 0 y 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para escalar las features, vamos a usar la clase MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Va a ser necesario crear un assembler por cada variable:<br>\n",
    "No es como antes que podemos meterle todas las columnas y sacar un solo vector, porque cada columna va a necesitar su Scaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que antes. Creamos el Assembler, y transformamos el df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_assembler = VectorAssembler(inputCols=['Ingresos'], outputCol=\"vec_Ingresos\")\n",
    "ing_assembled = ing_assembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá tenemos un df intermedio \"ing_assembled\"<br>\n",
    "Ahora creamos el objeto MinMaxScaler, que va a transformar el df ing_assembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_scaler = MinMaxScaler(inputCol=\"vec_Ingresos\", outputCol=\"sc_Ingresos\")\n",
    "ing_scaler_model = ing_scaler.fit(ing_assembled)\n",
    "ing_df = ing_scaler_model.transform(ing_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así queda nuesta feature original, nuestra feature vectorizada y nuestra feature vectorizada y escalada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+\n",
      "|Ingresos|vec_Ingresos|         sc_Ingresos|\n",
      "+--------+------------+--------------------+\n",
      "|      49|      [49.0]|[0.1898148148148148]|\n",
      "|      34|      [34.0]|[0.12037037037037...|\n",
      "|      11|      [11.0]|[0.01388888888888...|\n",
      "|     100|     [100.0]|[0.42592592592592...|\n",
      "|      45|      [45.0]|[0.17129629629629...|\n",
      "+--------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ing_df.select('Ingresos', 'vec_Ingresos', 'sc_Ingresos').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que hacer lo mismo para las otras 3 features...<br>\n",
    "Vamos a necesitar una forma más eficiente de hacer esto<br><br>\n",
    "Hagamos uso de la clase Pipeline de Spark que nos permiten combinar funciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "columns_to_scale = ['Educacion', 'Tarjeta Credito', 'Valor Hipoteca']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la clase Pipeline podemos hacer todas las transformaciones juntas. <br>\n",
    "Nos permite:\n",
    "- Aprovechar la lazy evaluation de Spark\n",
    "- Deshacerme de los pipelines intermedios\n",
    "- Ahorrar algunas líneas de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean todos los assemblers\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=\"vec_\" + col) for col in columns_to_scale]\n",
    "# Se crean todos los scalers\n",
    "scalers = [MinMaxScaler(inputCol=\"vec_\" + col, outputCol=\"sc_\" + col) for col in columns_to_scale]\n",
    "# Se crea el pipeline. se definen las etapas del pipeline: primero todos los assemblers, después todos los scalers. \n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "# GRAN VENTAJA DEL PIPELINE: Me permite deshacerme de los dfs intermedios. \n",
    "scalerModel = pipeline.fit(ing_df)  # lo que hay que fittear son los scalers. \n",
    "scaledData = scalerModel.transform(ing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Edad',\n",
       " 'Experiencia',\n",
       " 'Ingresos',\n",
       " 'Cod_Postal',\n",
       " 'Familia',\n",
       " 'Tarjeta Credito',\n",
       " 'Educacion',\n",
       " 'Valor Hipoteca',\n",
       " 'Cta Comitente',\n",
       " 'Plazo Fijo',\n",
       " 'Prestamo',\n",
       " 'vec_Ingresos',\n",
       " 'sc_Ingresos',\n",
       " 'vec_Educacion',\n",
       " 'vec_Tarjeta Credito',\n",
       " 'vec_Valor Hipoteca',\n",
       " 'sc_Educacion',\n",
       " 'sc_Tarjeta Credito',\n",
       " 'sc_Valor Hipoteca']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------------------+---------+-------------+------------+\n",
      "|Tarjeta Credito|vec_Tarjeta Credito|  sc_Tarjeta Credito|Educacion|vec_Educacion|sc_Educacion|\n",
      "+---------------+-------------------+--------------------+---------+-------------+------------+\n",
      "|            1.6|              [1.6]|[0.16000000000000...|        1|        [1.0]|       [0.0]|\n",
      "|            1.5|              [1.5]|[0.15000000000000...|        1|        [1.0]|       [0.0]|\n",
      "|            1.0|              [1.0]|               [0.1]|        1|        [1.0]|       [0.0]|\n",
      "|            2.7|              [2.7]|              [0.27]|        2|        [2.0]|       [0.5]|\n",
      "|            1.0|              [1.0]|               [0.1]|        2|        [2.0]|       [0.5]|\n",
      "|            0.4|              [0.4]|[0.04000000000000...|        2|        [2.0]|       [0.5]|\n",
      "|            1.5|              [1.5]|[0.15000000000000...|        2|        [2.0]|       [0.5]|\n",
      "|            0.3|              [0.3]|              [0.03]|        3|        [3.0]|       [1.0]|\n",
      "|            0.6|              [0.6]|              [0.06]|        2|        [2.0]|       [0.5]|\n",
      "|            8.9|              [8.9]|[0.8900000000000001]|        3|        [3.0]|       [1.0]|\n",
      "+---------------+-------------------+--------------------+---------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.select('Tarjeta Credito', 'vec_Tarjeta Credito', 'sc_Tarjeta Credito',\n",
    "                  'Educacion', 'vec_Educacion', 'sc_Educacion').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien. Todo listo para entrenar el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separo los datos en un set de entrenamiento y de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = scaledData.randomSplit([0.8,0.2])  # 80% training - 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4020, 980)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Está bien esto?\n",
    "¿Qué representa mi set de testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage\n",
    "Cuando escalé las features, usé el set completo. El algoritmo tuvo acceso a información \"del futuro\". En este caso no afecta demasiado... pero el Data Leakage es algo importante a tener en cuenta <br>\n",
    "Como regla general:\n",
    "- Para mirar los datos, podemos usar todos\n",
    "- Para manipularlos, primero hay que separar el test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split - Scaling do over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primero el Split, después el Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "training_data, test_data = spark_df.randomSplit([0.8,0.2])  # 80% training - 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4009, 991)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>27</td>\n",
       "      <td>72</td>\n",
       "      <td>91711</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   4    35            9       100       94112        1              2.7   \n",
       "1   5    35            8        45       91330        4              1.0   \n",
       "2   7    53           27        72       91711        2              1.5   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \n",
       "0          2               0              0           0         0  \n",
       "1          2               0              0           0         0  \n",
       "2          2               0              0           0         0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.toPandas()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \n",
    "# Hacemos exactamente lo mismo que antes\n",
    "columns_to_scale = ['Ingresos', 'Educacion', 'Tarjeta Credito', 'Valor Hipoteca']\n",
    "\n",
    "# Se crean todos los assemblers IGUAL QUE ANTES\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=\"vec_\" + col) for col in columns_to_scale]\n",
    "# Se crean todos los scalers IGUAL QUE ANTES\n",
    "scalers = [MinMaxScaler(inputCol=\"vec_\" + col, outputCol=\"sc_\" + col) for col in columns_to_scale]\n",
    "# Se crea el pipeline. IGUAL QUE ANTES\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "\n",
    "scaler_model = pipeline.fit(training_data)\n",
    "# hago la normalización para los datos de training y los de test\n",
    "sc_training_df = scaler_model.transform(training_data)\n",
    "sc_test_df = scaler_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Experiencia</th>\n",
       "      <th>Ingresos</th>\n",
       "      <th>Cod_Postal</th>\n",
       "      <th>Familia</th>\n",
       "      <th>Tarjeta Credito</th>\n",
       "      <th>Educacion</th>\n",
       "      <th>Valor Hipoteca</th>\n",
       "      <th>Cta Comitente</th>\n",
       "      <th>Plazo Fijo</th>\n",
       "      <th>Prestamo</th>\n",
       "      <th>vec_Ingresos</th>\n",
       "      <th>vec_Educacion</th>\n",
       "      <th>vec_Tarjeta Credito</th>\n",
       "      <th>vec_Valor Hipoteca</th>\n",
       "      <th>sc_Ingresos</th>\n",
       "      <th>sc_Educacion</th>\n",
       "      <th>sc_Tarjeta Credito</th>\n",
       "      <th>sc_Valor Hipoteca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[100.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[2.7]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[0.42592592592592593]</td>\n",
       "      <td>[0.5]</td>\n",
       "      <td>[0.27]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[45.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[0.17129629629629628]</td>\n",
       "      <td>[0.5]</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>27</td>\n",
       "      <td>72</td>\n",
       "      <td>91711</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[72.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[1.5]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[0.2962962962962963]</td>\n",
       "      <td>[0.5]</td>\n",
       "      <td>[0.15000000000000002]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>93943</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[22.0]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[0.3]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[0.06481481481481481]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[0.03]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>180</td>\n",
       "      <td>93023</td>\n",
       "      <td>1</td>\n",
       "      <td>8.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[180.0]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[8.9]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[0.7962962962962963]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[0.8900000000000001]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Edad  Experiencia  Ingresos  Cod_Postal  Familia  Tarjeta Credito  \\\n",
       "0   4    35            9       100       94112        1              2.7   \n",
       "1   5    35            8        45       91330        4              1.0   \n",
       "2   7    53           27        72       91711        2              1.5   \n",
       "3   8    50           24        22       93943        1              0.3   \n",
       "4  10    34            9       180       93023        1              8.9   \n",
       "\n",
       "   Educacion  Valor Hipoteca  Cta Comitente  Plazo Fijo  Prestamo  \\\n",
       "0          2               0              0           0         0   \n",
       "1          2               0              0           0         0   \n",
       "2          2               0              0           0         0   \n",
       "3          3               0              0           0         0   \n",
       "4          3               0              0           0         1   \n",
       "\n",
       "  vec_Ingresos vec_Educacion vec_Tarjeta Credito vec_Valor Hipoteca  \\\n",
       "0      [100.0]         [2.0]               [2.7]              [0.0]   \n",
       "1       [45.0]         [2.0]               [1.0]              [0.0]   \n",
       "2       [72.0]         [2.0]               [1.5]              [0.0]   \n",
       "3       [22.0]         [3.0]               [0.3]              [0.0]   \n",
       "4      [180.0]         [3.0]               [8.9]              [0.0]   \n",
       "\n",
       "             sc_Ingresos sc_Educacion     sc_Tarjeta Credito sc_Valor Hipoteca  \n",
       "0  [0.42592592592592593]        [0.5]                 [0.27]             [0.0]  \n",
       "1  [0.17129629629629628]        [0.5]                  [0.1]             [0.0]  \n",
       "2   [0.2962962962962963]        [0.5]  [0.15000000000000002]             [0.0]  \n",
       "3  [0.06481481481481481]        [1.0]                 [0.03]             [0.0]  \n",
       "4   [0.7962962962962963]        [1.0]   [0.8900000000000001]             [0.0]  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_training_df.toPandas()[:5]  # To Pandas es solo para visualización. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo, vamos a necesitar un vector similar al que usamos antes para la matriz de correlación. <br>\n",
    "Primero vamos a necesitar pasar las features normalizadas de vector a numero.<br>\n",
    "Spark parece muy complicado... comparación con Pandas. (exploratorio vs productivo. Todo lo que se hace acá es para mostrar, pero se podría hacer en un pipeline sin necesidad de mostrar resultados intermedios. Spark no remplaza a Pandas, son complementarios.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se pasan las features normalizadas de vector a float...\n",
    "# Para poder transformarlas en un solo vector. \n",
    "from pyspark.ml.functions import vector_to_array\n",
    "sc_training_df.withColumn(\"num_sc_Income\", vector_to_array(\"sc_Income\")[0])\\\n",
    ".withColumn('num_sc_Education', vector_to_array('sc_Education')[0])\\\n",
    ".withColumn('num_sc_CCAvg', vector_to_array('sc_CCAvg')[0])\\\n",
    ".withColumn('num_sc_Mortgage', vector_to_array('sc_Mortgage')[0])\\\n",
    ".select('ID', \"num_sc_Income\", \"num_sc_Education\", \"num_sc_CCAvg\", \"num_sc_Mortgage\", \"CD Account\", \"Personal Loan\")\\\n",
    ".toPandas()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sc_training_df.withColumn(\"num_sc_Income\", vector_to_array(\"sc_Income\")[0])\\\n",
    ".withColumn('num_sc_Education', vector_to_array('sc_Education')[0])\\\n",
    ".withColumn('num_sc_CCAvg', vector_to_array('sc_CCAvg')[0])\\\n",
    ".withColumn('num_sc_Mortgage', vector_to_array('sc_Mortgage')[0])\\\n",
    ".select('ID', \"num_sc_Income\", \"num_sc_Education\", \"num_sc_CCAvg\", \"num_sc_Mortgage\", \"CD Account\", \"Personal Loan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in training_set.columns if (col not in ['ID', 'Personal Loan'] )]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vec_features\")\n",
    "# transformar sc_training_df en algo que tenga una columna más. \n",
    "vec_training_set = vec_assembler.transform(training_set).select('vec_features', 'Personal Loan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_training_set.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Personal Loan\", featuresCol=\"vec_features\", maxIter=10)\n",
    "lr_pipeline = Pipeline(stages=[lr])  # Para una sola transformación, no es necesario el pipeline. \n",
    "# Pero, se puede combinar con la transformación anterior (columnas a vectores, para hacer el pipeline)\n",
    "lr_model = lr_pipeline.fit(vec_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos el test set de la misma manera que transformamos el anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = sc_test_df.withColumn(\"num_sc_Income\", vector_to_array(\"sc_Income\")[0])\\\n",
    ".withColumn('num_sc_Education', vector_to_array('sc_Education')[0])\\\n",
    ".withColumn('num_sc_CCAvg', vector_to_array('sc_CCAvg')[0])\\\n",
    ".withColumn('num_sc_Mortgage', vector_to_array('sc_Mortgage')[0])\\\n",
    ".select('ID', \"num_sc_Income\", \"num_sc_Education\", \"num_sc_CCAvg\", \"num_sc_Mortgage\", \"CD Account\", \"Personal Loan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_test_set = vec_assembler.transform(test_set).select('vec_features', 'Personal Loan')\n",
    "vec_test_set.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = lr_model.transform(vec_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('vec_features', 'rawPrediction', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.where(predictions['prediction'] == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.where(predictions['prediction'] == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces... Esto es bueno? Esto es malo... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué es lo que no estamos viendo acá? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Si la predicción es correcta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('vec_features', 'rawPrediction', 'prediction', 'Personal Loan').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué es? Para qué se usa? PowerPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Personal Loan\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "predictions_labels = predictions['prediction', 'Personal Loan']\\\n",
    ".withColumn('Personal Loan', col('Personal Loan').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_labels.rdd.map(tuple).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(predictions_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusionMatrix().toArray().transpose()\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix[1][1]\n",
    "tn = confusion_matrix[0][0]\n",
    "fp = confusion_matrix[1][0]\n",
    "fn = confusion_matrix[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy, Precision, Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregar a PowerPoint... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tp + tn) / (tp + tn + fp + fn)  # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94% suena bien, no?\n",
    "<br><br><br>..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el caso de un clasificador que siempre predice 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_positive = tp + fn\n",
    "total_negative = tn + fp\n",
    "total_cases = total_positive + total_negative\n",
    "\n",
    "print('Dummy accuracy: {}'.format((total_negative / (total_cases))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces: Nuestro clasificador es bueno, pero no taaan bueno. \n",
    "<br> Moraleja: No juzgar porcentajes... hay que ver todas las métricas\n",
    "<br> Otras métricas: \n",
    "- Precision\n",
    "- Recall\n",
    "<br> La métrica para clasificadores binarios es el área bajo la ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision(1.0)\n",
    "precision\n",
    "# busco minimizar fp (no encarcelar inocentes, aunque queden ladrones libres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tp) / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = metrics.recall(1.0)\n",
    "recall\n",
    "# busco minimizar fn (encarcelar todos los ladrones, a expensas de encarcelar algún inocente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tp) / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo: Minority Report - Ladrones e Inocentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curve (AUC ROC)\n",
    "Hay que explicar un poco qué son. Agregar a PowerPoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "bin_metrics = BinaryClassificationMetrics(predictions_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77% No es excelente, pero no está mal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL)\n",
    "## Ethics in Data Science and ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
